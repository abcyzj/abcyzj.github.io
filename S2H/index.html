<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Semantics2Hands: Transferring Hand Motion Semantics between Avatars">
  <meta property="og:title" content="Semantics2Hands" />
  <meta property="og:description" content="Semantics2Hands: Transferring Hand Motion Semantics between Avatars" />
  <meta property="og:url" content="https://abcyzj.github.io/S2H" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Semantics2Hands">
  <meta name="twitter:description" content="Semantics2Hands: Transferring Hand Motion Semantics between Avatars">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Hand Motion Retargeting">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Semantics2Hands</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Semantics2Hands: Transferring Hand Motion Semantics between Avatars
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://abcyzj.github.io/" target="_blank">Zijie Ye</a>,</span>
              <span class="author-block">
                <a href="https://hcsi.cs.tsinghua.edu.cn/" target="_blank">Jia Jia,</span>
              <span class="author-block">
                <a href="https://www.cs.tsinghua.edu.cn/info/1116/5088.htm" target="_blank">Junliang Xing</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Tsinghua University<br>ACM MM 2023</span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/S2H-camera.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Data link -->
                <span class="link-block">
                  <a href="https://drive.google.com/file/d/1uZtYpYwFG0Z-tWuvYcwULRGDF14ebqK9/view?usp=sharing"
                    target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/abcyzj/Semantics2Hands" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2308.05920" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/videos/teaser.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Given a source hand motion and a target hand model, our method can retarget realistic hand motions with high
          fidelity to the target while preserving intricate motion semantics.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Human hands, the primary means of non-verbal communication, convey intricate semantics in various
              scenarios. Due to the high sensitivity of individuals to hand motions, even minor errors in hand motions
              can significantly impact the user experience. Real applications often involve multiple avatars with
              varying hand shapes, highlighting the importance of maintaining the intricate semantics of hand motions
              across the avatars.
              Therefore, this paper aims to transfer the hand motion semantics between diverse avatars based on their
              respective hand models. To address this problem, we introduce a novel anatomy-based semantic matrix (ASM)
              that encodes the semantics of hand motions. The ASM quantifies the positions of the palm and other joints
              relative to the local frame of the corresponding joint, enabling precise retargeting of hand motions.
              Subsequently, we obtain a mapping function from the source ASM to the target hand joint rotations by
              employing an anatomy-based semantics reconstruction network (ASRN). We train the ASRN using a
              semi-supervised learning strategy on the Mixamo and InterHand2.6M datasets. We evaluate our method in
              intra-domain and cross-domain hand motion retargeting tasks. The qualitative and quantitative results
              demonstrate the significant superiority of our ASRN over the state-of-the-arts.
            </p>
          </div>
          <img src="static/images/motivation.png" width="80%" class="center-image" />
          <h2 class="subtitle has-text-centered">Despite the accurate body motions, errors introduced by copying finger
            joint rotations make the "thumb-up" gesture illegible.</h2>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="content">
        <h2 class="title is-3">Twist-bend-splay Frame Annotation</h2>
        <div class="level-set has-text-justified">
          We first use our annotation tool to annotate the twist-bend-splay frames of different hand models. The
          annotation tool can semi-automatically derive the frame orientation of finger joints for Twist-bend-splay from
          the model's kinematic tree and mesh information.
        </div>
      </div>
      <img class="center-image" src="static/images/tbs.png" width="70%" />
      <h2 class="subtitle has-text-centered">
        Left: Twist-bend-splay frames obtained from different hand models using our annotation tool. <br />
        Right: Finger movements in the twist, splay, and bend directions.
      </h2>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="content">
        <h2 class="title is-3">Anatomy-based Semantic Matrix</h2>
        <div class="level-set has-text-justified">
          We then use the twist-bend-splay frames to construct the anatomy-based semantic matrix (ASM). The ASM
          quantifies the positions of the palm and other joints relative to the local frame of the corresponding joint,
          enabling precise retargeting of hand motions.
        </div>
      </div>
      <img class="center-image" src="static/images/matrix.png" width="80%" />
      <h2 class="subtitle has-text-centered">
        Left: The inter-finger semantic features capture the subtle semantics of finger movements. <br />
        Right: The palm-finger semantic features capture the overall hand posture.
      </h2>
    </div>
  </section>

  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="content">
        <h2 class="title is-3">Semantics-Preserving Retargeting</h2>
        <div class="level-set has-text-justified">
          The hand retargeting pipeline comprises two stages: semantic feature extraction and semantics-preserving
          reconstruction. We extract semantic matrices from the source hand motion during the first stage. In the second
          stage, we employ the anatomy-based semantics reconstruction network (ASRN) to reconstruct hand motion on the
          target hand model from the source ASM while preserving the source semantics.
        </div>
      </div>
      <img class="center-image" src="static/images/overview.png" />
      <h2 class="subtitle has-text-centered">
        The extraction stage involves the retrieval of ASM from the source hand motion. The reconstruction stage
        utilizes the source ASM, target hand shape parameter, and target hand anatomical parameter to reconstruct the
        target hand motion.
      </h2>
    </div>
  </section>

  <!-- Image carousel -->
  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/comparison.png" width="80%" class="center-image" />
            <h2 class="subtitle has-text-centered">
              Qualitative comparison between the proposed framework and the state-of-the-art methods.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/detail.png" width="80%" class="center-image" />
            <h2 class="subtitle has-text-centered">
              Our framework maintains precise spatial relationships among the fingers.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
<pre><code>
@inproceedings{ye2023semantics,
  title={Semantics2Hands: Transferring Hand Motion Semantics between Avatars},
  author={Ye, Zijie and Jia, Jia and Xing, Junliang},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  year={2023}
}
</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a>.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>